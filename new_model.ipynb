{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import anndata as ad\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import poisson\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import psutil\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = psutil.Process(os.getpid())\n",
    "p.cpu_affinity([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, path, num_patches):\n",
    "        \"\"\"\n",
    "        Custom dataset for loading synthetic spatial transcriptomics data.\n",
    "\n",
    "        Args:\n",
    "            path (str): The base directory where the data is stored.\n",
    "            num_patches (int): Number of patches (datasets) to load.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.num_patches = num_patches\n",
    "        self.data = []\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the synthetic data from the specified path.\n",
    "        \"\"\"\n",
    "        # for i in range(self.num_patches):\n",
    "        #     # Load the .npz file containing the coordinates and gene indices\n",
    "        #     data = np.load(os.path.join(self.path, 'data', f'gene_coordinates_{i}.npz'))\n",
    "        #     x_coords = torch.tensor(data['x_coords'], dtype=torch.long)\n",
    "        #     y_coords = torch.tensor(data['y_coords'], dtype=torch.long)\n",
    "        #     gene_indices = torch.tensor(data['gene_indices'], dtype=torch.long)\n",
    "        #     # Load the corresponding cell positions from the pickle file\n",
    "        #     # with open(os.path.join(self.path, 'cell_positions', f'cell_{i}.pkl'), 'rb') as f:\n",
    "        #     #     cell_positions = pickle.load(f)\n",
    "\n",
    "        #     # Store the data as a tuple\n",
    "        #     self.data.append((x_coords, y_coords, gene_indices))#cell_positions))\n",
    "            \n",
    "        data = np.load('/data/aram/Xenium/output-XETG00056__0004637__Region_1__20230718__204100/code/data/synthetic_data_2/gene_coordinates_test_cells1.npz')\n",
    "        x_coords = torch.tensor(data['x_coords'], dtype=torch.long)\n",
    "        y_coords = torch.tensor(data['y_coords'], dtype=torch.long)\n",
    "        gene_indices = torch.tensor(data['gene_indices'], dtype=torch.long)\n",
    "        self.data.append((x_coords, y_coords, gene_indices))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of patches (datasets).\n",
    "        \"\"\"\n",
    "        return self.num_patches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a specific dataset item by index.\n",
    "        Args:\n",
    "            idx (int): The index of the dataset item.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing x-coordinates, y-coordinates, gene indices, and cell positions.\n",
    "        \"\"\"\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_coords(coords):\n",
    "    coords = coords.float()\n",
    "    mean = torch.mean(coords)\n",
    "    std = torch.std(coords)\n",
    "    return (coords - mean) / std\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create the positional encodings\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, pos_indices):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            pos_indices: Tensor of shape ``[seq_len]``, containing the positional indices.\n",
    "        Returns:\n",
    "            Tensor of shape ``[seq_len, d_model]`` containing the corresponding positional encodings.\n",
    "        \"\"\"\n",
    "        return self.pe[pos_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneEmbedding(nn.Module):\n",
    "    def __init__(self, n_genes, embedding_dim):\n",
    "        super(GeneEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=n_genes, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, gene_indices):\n",
    "        return self.embedding(gene_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, nhead, num_layers, dim_feedforward):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEArgmax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Perform the argmax to get the index of the maximum value\n",
    "        indices = torch.argmax(input, dim=-1)\n",
    "        # Create a one-hot encoded tensor with the same shape as the input\n",
    "        one_hot_output = F.one_hot(indices, num_classes=input.shape[-1]).float()\n",
    "        ctx.save_for_backward(input)\n",
    "        return one_hot_output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # In the straight-through estimator, we return the gradient as-is\n",
    "        return grad_output,  # Return as a tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullPipeline(nn.Module):\n",
    "    def __init__(self, height_dim, width_dim, gene_dim, n_genes, n_cells, nhead, num_layers, dim_feedforward):\n",
    "        super(FullPipeline, self).__init__()\n",
    "        embedding_dim = height_dim #+ width_dim + gene_dim\n",
    "        self.d_model= height_dim\n",
    "        self.gene_embedding = GeneEmbedding(n_genes, gene_dim)\n",
    "        self.transformer_model = TransformerModel(embedding_dim, nhead, num_layers, dim_feedforward)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, n_cells)\n",
    "        )\n",
    "        \n",
    "        self.pe1 = PositionalEncoding(height_dim)\n",
    "        self.pe2 = PositionalEncoding(width_dim)\n",
    "        self.temperature = 1\n",
    "\n",
    "    def forward(self, height_coords, width_coords, gene_indices):\n",
    "        positional_height = self.pe1(height_coords)#.to(device)\n",
    "        positional_width = self.pe2(width_coords)#.to(device)\n",
    "        gene_encoded = self.gene_embedding(gene_indices)#.to(device)\n",
    "        \n",
    "        encoded_input = positional_height + positional_width + gene_encoded\n",
    "        \n",
    "        transformer_output = self.transformer_model(encoded_input)\n",
    "        \n",
    "        output = self.ffn(transformer_output)\n",
    "        output = nn.Softmax(dim=2)(output / self.temperature)\n",
    "        \n",
    "        # final_output = STEArgmax.apply(output).squeeze(0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance_loss(height_coords, width_coords, output_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the differentiable pairwise distance loss based on height and width coordinates.\n",
    "    \n",
    "    Args:\n",
    "        height_coords (torch.Tensor): Tensor of shape (n_samples,) containing standardized height coordinates.\n",
    "        width_coords (torch.Tensor): Tensor of shape (n_samples,) containing standardized width coordinates.\n",
    "        output_matrix (torch.Tensor): Tensor of shape (n_samples, 5) representing the softmax output from the model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed pairwise distance loss (scalar).\n",
    "    \"\"\"\n",
    "    device = output_matrix.device\n",
    "    height_coords = height_coords.to(device)\n",
    "    width_coords = width_coords.to(device)\n",
    "\n",
    "    coords = torch.stack((height_coords.squeeze(0), width_coords.squeeze(0)), dim=1).float()  # Shape: (n_samples, 2)\n",
    "    pairwise_distances = torch.cdist(coords, coords)\n",
    "    # print(output_matrix)  \n",
    "    mask = torch.matmul(output_matrix, output_matrix.T)\n",
    "    # print(mask)\n",
    "    masked_distances = mask * pairwise_distances\n",
    "    total_loss = masked_distances.sum()\n",
    "    \n",
    "    # prob_outer = output_matrix.unsqueeze(2) * output_matrix.unsqueeze(1)  # Shape: (1000, 5, 1000)\n",
    "\n",
    "    # total_loss = torch.sum(pairwise_distances.unsqueeze(1) * prob_outer)\n",
    "    n_samples = height_coords.size(0)\n",
    "    normalized_loss = total_loss / n_samples\n",
    "    \n",
    "    return normalized_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/aram/Xenium/output-XETG00056__0004637__Region_1__20230718__204100/code/data/synthetic_data_2/cell_type_profiles.pkl', 'rb') as file:\n",
    "    cell_type_profiles_np = pickle.load(file)\n",
    "    cell_type_profiles = torch.tensor(cell_type_profiles_np, dtype=torch.float32).to(device)\n",
    "\n",
    "def one_hot_encode(gene_indices, n_genes):\n",
    "    \"\"\"\n",
    "    Converts gene indices into a one-hot encoded matrix.\n",
    "    \n",
    "    Args:\n",
    "        gene_indices (torch.Tensor): Tensor of shape (n_samples,) containing gene indices.\n",
    "        n_genes (int): The number of unique genes.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: One-hot encoded matrix of shape (n_samples, n_genes).\n",
    "    \"\"\"\n",
    "    return F.one_hot(gene_indices, num_classes=n_genes).float()\n",
    "\n",
    "def compute_log_likelihood(expression_vector, cell_type_profile, umi_count):\n",
    "    lambdas = umi_count * cell_type_profile\n",
    "    # Compute Poisson log-likelihood: log P(v_i | λ_i)\n",
    "    # Poisson log-likelihood: v_i * log(λ_i) - λ_i - log(v_i!)\n",
    "    log_likelihood = expression_vector * torch.log(lambdas + 1e-8) - lambdas - torch.lgamma(expression_vector + 1)\n",
    "    \n",
    "    # Sum the log-probabilities across all genes to get the total log-likelihood\n",
    "    total_log_likelihood = torch.sum(log_likelihood)\n",
    "    \n",
    "    average_log_likelihood = total_log_likelihood / umi_count\n",
    "    \n",
    "    return average_log_likelihood\n",
    "    \n",
    "\n",
    "def likelihood_model(expression_vector, temperature):\n",
    "    \"\"\"\n",
    "    Takes in each row (cell) and computes a score.\n",
    "\n",
    "    Args:\n",
    "        expression_vector (torch.Tensor): Tensor of shape (n_genes,) representing gene counts for a cell.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar likelihood score for the cell.\n",
    "    \"\"\"\n",
    "    observed_umi_count = torch.sum(expression_vector)\n",
    "\n",
    "    log_likelihoods = torch.stack([\n",
    "        compute_log_likelihood(expression_vector, cell_type_profile, observed_umi_count)\n",
    "        for cell_type_profile in cell_type_profiles\n",
    "    ])\n",
    "    log_likelihoods_ = log_likelihoods *1\n",
    "    \n",
    "    # return torch.max(log_likelihoods)\n",
    "    prob = F.softmax(log_likelihoods_/temperature, dim=0)\n",
    "    # print(log_likelihoods, \"log_likelihoods\")\n",
    "    # print(prob, \"prob\")\n",
    "    # max_index = torch.argmax(prob)\n",
    "    # max_value = prob[max_index]\n",
    "    # print(f\"Cell type: {max_index.item()}, Prob: {max_value.item()}\")\n",
    "    # print(log_likelihoods, \"log_likelihoods\")\n",
    "    # print(prob*log_likelihoods, \"prob*log_likelihoods\")\n",
    "    summed_likelihood = torch.sum(prob*log_likelihoods_)\n",
    "    # print(summed_likelihood, \"summed_likelihood\")\n",
    "    return -summed_likelihood#torch.sum(prob * torch.log(prob + 1e-10), dim=0).mean()\n",
    "\n",
    "def likelihood_loss(output_matrix, gene_indices, n_genes, temperature):\n",
    "    \"\"\"\n",
    "    Calculate the likelihood loss based on the gene index one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        output_matrix (torch.Tensor): Tensor of shape (n_samples, 5) representing the softmax output from the model.\n",
    "        gene_indices (torch.Tensor): Tensor of shape (n_samples,) containing gene indices.\n",
    "        n_genes (int): The number of unique genes.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed likelihood loss (scalar).\n",
    "    \"\"\"\n",
    "    print(temperature, \"temperature\")\n",
    "\n",
    "    one_hot_genes = one_hot_encode(gene_indices, n_genes)  # Shape: (n_samples, n_genes)\n",
    "    cell_by_gene = torch.matmul(output_matrix.T, one_hot_genes).squeeze(0)  # Shape: (5, n_genes)\n",
    "    total_loss = 0.0\n",
    "    for cell in cell_by_gene:\n",
    "        total_loss += likelihood_model(cell, temperature)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_loss(output_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the background loss, which is the number of transcripts assigned to the last cell class.\n",
    "    \n",
    "    Args:\n",
    "        output_matrix (torch.Tensor): Tensor of shape (n_samples, 5) representing the softmax output from the model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The background loss (scalar), which is the sum of all probabilities assigned to the last cell class.\n",
    "    \"\"\"\n",
    "    last_cell_probs = output_matrix[:, -1]  # Shape: (n_samples,)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    total_background_loss = last_cell_probs.sum()\n",
    "    return total_background_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, scheduler, lambda_, n_genes, num_epochs=5):\n",
    "    temperature = 1.0\n",
    "    min_temperature = 0.001\n",
    "    N = 1\n",
    "    r = 1e-5  \n",
    "    global_steps = 0\n",
    "    model.train() \n",
    "    global_steps = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        global_steps += 1\n",
    "        epoch_loss = 0\n",
    "        epoch_loss1 = 0\n",
    "        epoch_loss2 = 0\n",
    "        epoch_loss3 = 0\n",
    "        for batch in dataloader:\n",
    "            \n",
    "            # if global_steps % N == 0:\n",
    "            #     temperature = max(min_temperature, math.exp(-r * global_steps*1000))\n",
    "            temperature = 1\n",
    "                \n",
    "            height_coords, width_coords, gene_indices = [x.to(device) for x in batch]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output_matrix = model(height_coords, width_coords, gene_indices)\n",
    "            output_matrix = output_matrix.squeeze(0)\n",
    "            # output_matrix.retain_grad()\n",
    "            \n",
    "            # cells_matrix = output_matrix[:, :-1]\n",
    "            pairwise_distance = 0#pairwise_distance_loss(height_coords, width_coords, output_matrix)\n",
    "            likelihood = likelihood_loss(output_matrix, gene_indices, n_genes, temperature)\n",
    "            background = 0#background_loss(output_matrix)\n",
    "            \n",
    "            loss1 = lambda_[0]*pairwise_distance\n",
    "            loss2 = lambda_[1]*likelihood \n",
    "            loss3 = lambda_[2]*background\n",
    "\n",
    "            total_loss = loss1 + loss2 + loss3\n",
    "            \n",
    "            # total_loss = total_loss/1000\n",
    "    \n",
    "            total_loss.backward()\n",
    "            # print(output_matrix.grad)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "            epoch_loss1 += loss1\n",
    "            epoch_loss2 += loss2\n",
    "            epoch_loss3 += loss3\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], total Loss: {epoch_loss:.4f}')\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss 1: {epoch_loss1:.4f}')\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss 2: {epoch_loss2:.4f}')\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss 3: {epoch_loss3:.4f}')\n",
    "    return output_matrix, height_coords, width_coords, gene_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/aram/Xenium/output-XETG00056__0004637__Region_1__20230718__204100/code/data/synthetic_data_2\"\n",
    "num_patches = 1\n",
    "lambda_ = (0.05, 100, 0)\n",
    "n_genes = 30\n",
    "\n",
    "dataset = GeneDataset(path, num_patches)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model = FullPipeline(\n",
    "height_dim=256, width_dim=256, gene_dim=256, n_genes=30, n_cells=5, nhead=2, num_layers=6, dim_feedforward=500).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_warmup_steps = 100  \n",
    "num_training_steps = 1000  \n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_cycles=0.5 \n",
    ")\n",
    "\n",
    "output_matrix, height_coords, width_coords, gene_indices = train_model(model, dataloader, optimizer, scheduler, lambda_, n_genes, num_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.01\n",
    "one_hot_genes = one_hot_encode(gene_indices, n_genes)  # Shape: (n_samples, n_genes)\n",
    "cell_by_gene = torch.matmul(output_matrix.T, one_hot_genes).squeeze(0)  # Shape: (5, n_genes)\n",
    "total_loss = 0.0\n",
    "cell_num = -1\n",
    "\n",
    "loss_values = {}\n",
    "\n",
    "for cell in cell_by_gene:\n",
    "    cell_num +=1\n",
    "    print(cell_num, \"cell_num\")\n",
    "    observed_umi_count = torch.sum(cell)\n",
    "\n",
    "    log_likelihoods = torch.stack([\n",
    "        compute_log_likelihood(cell, cell_type_profile, observed_umi_count)\n",
    "        for cell_type_profile in cell_type_profiles\n",
    "    ])\n",
    "    prob = F.softmax(log_likelihoods/temperature, dim=0)\n",
    "    print(prob,\"prob\")\n",
    "    print(log_likelihoods,\"log_likelihoods\")\n",
    "    summed_likelihood = torch.sum(prob*log_likelihoods)\n",
    "    print(summed_likelihood,\"summed_likelihood\")\n",
    "    loss_values[cell_num] = -summed_likelihood\n",
    "    total_loss -= summed_likelihood#torch.sum(prob * torch.log(prob + 1e-10), dim=0).mean()\n",
    "print(total_loss,\"total_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(loss_values.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Ensure unique genes are identified and a color map is created\n",
    "unique_genes = np.unique(gene_indices.cpu())\n",
    "n_genes = len(unique_genes)\n",
    "cmap = ListedColormap(sns.color_palette(\"tab20\", n_genes))\n",
    "\n",
    "# Identify the most likely cell each transcript belongs to\n",
    "indices = torch.argmax(output_matrix, dim=-1)\n",
    "one_hot_output = F.one_hot(indices, num_classes=output_matrix.shape[-1]).float()\n",
    "transcript_by_cell = one_hot_output.squeeze(0)\n",
    "\n",
    "# Number of cells\n",
    "n_cells = transcript_by_cell.shape[1]\n",
    "\n",
    "# Create a list to hold patches of each cell\n",
    "cell_patches = [np.zeros((100, 100)) for _ in range(n_cells)]\n",
    "\n",
    "# Populate cell patches with gene indices\n",
    "for i in range(len(gene_indices[0, :])):\n",
    "    cell_idx = transcript_by_cell[i].nonzero(as_tuple=True)[0].item()\n",
    "    x, y = height_coords[0, i].item(), width_coords[0, i].item()\n",
    "    gene = gene_indices[0, i].item()\n",
    "\n",
    "    if 0 <= x < 100 and 0 <= y < 100:\n",
    "        cell_patches[cell_idx][y, x] = gene\n",
    "\n",
    "# Determine grid size based on the number of cells\n",
    "grid_size = int(np.ceil(np.sqrt(n_cells)))\n",
    "fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "\n",
    "# Loop through each cell and plot it in the grid\n",
    "for cell_idx in range(n_cells):\n",
    "    row = cell_idx // grid_size\n",
    "    col = cell_idx % grid_size\n",
    "    \n",
    "    sns.heatmap(cell_patches[cell_idx], cmap=cmap, cbar=False, vmin=0, vmax=n_genes - 1, ax=axs[row, col])\n",
    "    loss = loss_values[cell_idx]  # Default to 0 if no loss value\n",
    "    axs[row, col].set_title(f\"Cell {cell_idx} (Loss: {loss:.4f})\")\n",
    "    axs[row, col].axis('off')\n",
    "\n",
    "# Remove empty subplots if n_cells doesn't fill the grid\n",
    "for i in range(n_cells, grid_size * grid_size):\n",
    "    fig.delaxes(axs.flatten()[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "unique_genes = np.unique(gene_indices.cpu())\n",
    "n_genes = len(unique_genes)\n",
    "\n",
    "cmap = ListedColormap(sns.color_palette(\"tab20\", n_genes))\n",
    "\n",
    "indices = torch.argmax(output_matrix, dim=-1)\n",
    "one_hot_output = F.one_hot(indices, num_classes=output_matrix.shape[-1]).float()\n",
    "transcript_by_cell = one_hot_output.squeeze(0)\n",
    "\n",
    "# Number of cells\n",
    "n_cells = transcript_by_cell.shape[1]\n",
    "\n",
    "cell_patches = [np.zeros((100, 100)) for _ in range(n_cells)]\n",
    "\n",
    "for i in range(len(gene_indices[0,:])):\n",
    "    # Get the cell that this transcript is associated with\n",
    "    cell_idx = transcript_by_cell[i].nonzero(as_tuple=True)[0].item()\n",
    "    x, y = height_coords[0,i].item(), width_coords[0,i].item()\n",
    "    gene = gene_indices[0,i].item()\n",
    "    \n",
    "    if 0 <= x < 100 and 0 <= y < 100:\n",
    "        cell_patches[cell_idx][y, x] = gene\n",
    "\n",
    "for cell_idx in range(n_cells):\n",
    "    sns.heatmap(cell_patches[cell_idx], cmap=cmap, cbar=True, vmin=0, vmax=n_genes - 1)  # Added colormap for better visualization\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"Gradient for {name}:\")\n",
    "        print(param.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         plt.hist(param.detach().cpu().numpy(), bins=50)\n",
    "#         plt.title(f'Weight distribution for {name}')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Open and read the text file\n",
    "with open('/data/aram/Xenium/output-XETG00056__0004637__Region_1__20230718__204100/loss.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Parsing the data\n",
    "epochs = []\n",
    "total_loss = []\n",
    "loss_1 = []\n",
    "loss_2 = []\n",
    "\n",
    "lines = data.strip().split('\\n')\n",
    "for i in range(1, len(lines), 5):\n",
    "    epoch_info = lines[i].split(',')\n",
    "    epoch_num = int(epoch_info[0].split('[')[1].split('/')[0])\n",
    "    epochs.append(epoch_num)\n",
    "    \n",
    "    total_loss_val = float(epoch_info[1].split(': ')[1])\n",
    "    total_loss.append(total_loss_val)\n",
    "    \n",
    "    loss_1_val = float(lines[i+1].split(': ')[1])\n",
    "    loss_1.append(loss_1_val)\n",
    "    \n",
    "    loss_2_val = float(lines[i+2].split(': ')[1])\n",
    "    loss_2.append(loss_2_val)\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, total_loss, label='Total Loss')\n",
    "plt.plot(epochs, loss_1, label='Loss 1')\n",
    "plt.plot(epochs, loss_2, label='Loss 2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Losses over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
